{
  "cells": [
    {
      "cell_type": "code",
      "execution_count": 15,
      "id": "730f3575",
      "metadata": {
        "id": "730f3575"
      },
      "outputs": [],
      "source": [
        "import tensorflow as tf\n",
        "import numpy as np\n",
        "import joblib"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 16,
      "id": "7OzH8c7XpcAk",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "7OzH8c7XpcAk",
        "outputId": "a9580333-d228-4883-9de7-2799399d9a6e"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Drive already mounted at /content/drive; to attempt to forcibly remount, call drive.mount(\"/content/drive\", force_remount=True).\n"
          ]
        }
      ],
      "source": [
        "from google.colab import drive\n",
        "drive.mount('/content/drive')"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "y16n4_PxT-1f",
      "metadata": {
        "id": "y16n4_PxT-1f"
      },
      "source": [
        "# Setup TF Record Parser"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 17,
      "id": "7a28521c",
      "metadata": {
        "id": "7a28521c"
      },
      "outputs": [],
      "source": [
        "# parse single tf record\n",
        "def parse_tfrecord(file):\n",
        "    feature_description={\n",
        "        'note' : tf.io.FixedLenFeature([], tf.int64),\n",
        "        'note_str' : tf.io.FixedLenFeature([], tf.string),\n",
        "        'instrument' : tf.io.FixedLenFeature([], tf.int64),\n",
        "        #'instrument_str' : tf.io.FixedLenFeature([], tf.string),\n",
        "        'pitch': tf.io.FixedLenFeature([], tf.int64),\n",
        "        'velocity': tf.io.FixedLenFeature([], tf.int64),\n",
        "        'sample_rate': tf.io.FixedLenFeature([], tf.int64),\n",
        "        'audio': tf.io.FixedLenFeature([64000], tf.float32),\n",
        "        'qualities': tf.io.FixedLenFeature([10], tf.int64),\n",
        "        #'qualities_str': tf.io.VarLenFeature(tf.string),\n",
        "        'instrument_family': tf.io.FixedLenFeature([], tf.int64),\n",
        "        #'instrument_family_str': tf.io.FixedLenFeature([], tf.string),\n",
        "        'instrument_source': tf.io.FixedLenFeature([], tf.int64),\n",
        "        #'instrument_source_str': tf.io.FixedLenFeature([], tf.string)\n",
        "    }\n",
        "\n",
        "    return tf.io.parse_single_example(file, feature_description)\n"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "A2Wg3XtqUlth",
      "metadata": {
        "id": "A2Wg3XtqUlth"
      },
      "source": [
        "# Parsing TF Records, Saving Datasets"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "m_Ple1JTarJJ",
      "metadata": {
        "id": "m_Ple1JTarJJ"
      },
      "source": [
        "## Intake Training TFRecord"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 18,
      "id": "avHVYKKVUkJg",
      "metadata": {
        "id": "avHVYKKVUkJg"
      },
      "outputs": [],
      "source": [
        "from collections import defaultdict\n",
        "\n",
        "# intake training dataset\n",
        "trainDS_raw = tf.data.TFRecordDataset(\"/content/drive/MyDrive/nsynth-train.tfrecord\")\n",
        "\n",
        "# map parsing function to dataset\n",
        "trainingDS = trainDS_raw.map(parse_tfrecord)\n",
        "\n",
        "from sklearn.decomposition import IncrementalPCA\n",
        "import numpy as np\n",
        "\n",
        "\n"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "zAen6cdgauo7",
      "metadata": {
        "id": "zAen6cdgauo7"
      },
      "source": [
        "## Intake Validation TFRecord"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 19,
      "id": "SR-D2XngG7Tk",
      "metadata": {
        "id": "SR-D2XngG7Tk"
      },
      "outputs": [],
      "source": [
        "# intake validation TF Record\n",
        "\n",
        "validDS_raw = tf.data.TFRecordDataset(\"/content/drive/MyDrive/nsynth-valid.tfrecord\")\n",
        "\n",
        "# map parsing function to dataset\n",
        "validDS = validDS_raw.map(parse_tfrecord)\n"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "v5zqOGgGpS4M",
      "metadata": {
        "id": "v5zqOGgGpS4M"
      },
      "source": [
        "# Intake Test TFRecord"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 20,
      "id": "nuLvwr9zpVNF",
      "metadata": {
        "id": "nuLvwr9zpVNF"
      },
      "outputs": [],
      "source": [
        "# intake test TF Record\n",
        "\n",
        "testDS_raw = tf.data.TFRecordDataset(\"/content/drive/MyDrive/nsynth-test.tfrecord\")\n",
        "\n",
        "# map parsing function to dataset\n",
        "testDS = testDS_raw.map(parse_tfrecord)\n"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "nQ890j0lT0AF",
      "metadata": {
        "id": "nQ890j0lT0AF"
      },
      "source": [
        "## Establish Features and Target"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 21,
      "id": "76e68866",
      "metadata": {
        "id": "76e68866"
      },
      "outputs": [],
      "source": [
        "import time\n",
        "import csv\n",
        "from tqdm import tqdm\n",
        "import re\n",
        "import numpy as np\n",
        "import pandas as pd\n",
        "\n",
        "# get features and target\n",
        "# note target is 'instrument'\n",
        "# features are select other categories\n",
        "feat1 = ['note','instrument_source']\n",
        "feat2 = ['velocity','pitch']\n",
        "feat3 = ['audio']\n",
        "target = ['instrument_family']"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "qbLs66bqbTTV",
      "metadata": {
        "id": "qbLs66bqbTTV"
      },
      "source": [
        "## Get Record Counts"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 22,
      "id": "5b3OdS3bbYuy",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "5b3OdS3bbYuy",
        "outputId": "24c32764-09ef-4062-ef1b-f83858b94155"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Total Training Records: 289205\n",
            "Total Validation Records: 12678\n",
            "Total Test Records: 4096\n"
          ]
        }
      ],
      "source": [
        "totalTrainRecs = 289205\n",
        "totalValidRecs = 12678\n",
        "totalTestRecs = 4096\n",
        "print(f\"Total Training Records: {totalTrainRecs}\")\n",
        "print(f\"Total Validation Records: {totalValidRecs}\")\n",
        "print(f\"Total Test Records: {totalTestRecs}\")"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "akcBcnnVe-7l",
      "metadata": {
        "id": "akcBcnnVe-7l"
      },
      "source": [
        "# Convert Training Audio Arrays to Mel Spectograms"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 9,
      "id": "yG4wPWY7SV9O",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "yG4wPWY7SV9O",
        "outputId": "05400704-4221-411c-f03c-ade17c7cc758"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Converting Arrays to Spectograms: 100%|██████████| 289205/289205 [1:00:21<00:00, 79.85it/s] \n"
          ]
        }
      ],
      "source": [
        "# preprocess train audio arrays into mel spectograms\n",
        "import librosa\n",
        "for example in tqdm(trainingDS,total=totalTrainRecs,desc=\"Converting Arrays to Spectograms\"):\n",
        "#for sample in trainingDS.take(1):\n",
        "  audio = example['audio']\n",
        "  audio_np = audio.numpy()\n",
        "\n",
        "  #print(f\"Audio shape: {audio_np.shape}, dtype: {audio_np.dtype}\")\n",
        "  mel_spec = librosa.feature.melspectrogram(\n",
        "      y=audio_np,\n",
        "      sr=16000,\n",
        "      n_fft=1024,\n",
        "      hop_length=256,\n",
        "      n_mels=64\n",
        "  )\n",
        "  mel_spec_db = librosa.power_to_db(mel_spec, ref=np.max)\n",
        "  #print(mel_spec_db)\n",
        " # Min-max normalization to [0, 1]\n",
        "  min_val = mel_spec_db.min()\n",
        "  max_val = mel_spec_db.max()\n",
        "  mel_spec= (mel_spec_db - min_val) / (max_val - min_val + 1e-6)\n",
        "  #print(f\"Success! Mel shape: {mel_spec.shape}\")\n",
        "  #print(mel_spec)\n",
        "  mel_spec = mel_spec.flatten()\n",
        "  #print(f\"Success! Mel shape: {mel_spec.shape}\")\n",
        "  example['audio'] = mel_spec"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "_H0Su_5UfEc2",
      "metadata": {
        "id": "_H0Su_5UfEc2"
      },
      "source": [
        "# Convert Validation Audio Arrays to Mel Spectograms"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 10,
      "id": "EGAuTgakJD0X",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "EGAuTgakJD0X",
        "outputId": "0bfead05-e46c-4ce2-f8e5-781260f57b97"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Converting Arrays to Spectograms: 100%|██████████| 12678/12678 [02:25<00:00, 87.29it/s] \n"
          ]
        }
      ],
      "source": [
        "# preprocess valid audio arrays into mel spectograms\n",
        "import librosa\n",
        "\n",
        "for example in tqdm(validDS,total=totalValidRecs,desc=\"Converting Arrays to Spectograms\"):\n",
        "#for sample in trainingDS.take(1):\n",
        "  audio = example['audio']\n",
        "  audio_np = audio.numpy()\n",
        "\n",
        "  #print(f\"Audio shape: {audio_np.shape}, dtype: {audio_np.dtype}\")\n",
        "  mel_spec = librosa.feature.melspectrogram(\n",
        "      y=audio_np,\n",
        "      sr=16000,\n",
        "      n_fft=1024,\n",
        "      hop_length=256,\n",
        "      n_mels=64\n",
        "  )\n",
        "  mel_spec_db = librosa.power_to_db(mel_spec, ref=np.max)\n",
        "  #print(mel_spec_db)\n",
        " # Min-max normalization to [0, 1]\n",
        "  min_val = mel_spec_db.min()\n",
        "  max_val = mel_spec_db.max()\n",
        "  mel_spec= (mel_spec_db - min_val) / (max_val - min_val + 1e-6)\n",
        "  #print(f\"Success! Mel shape: {mel_spec.shape}\")\n",
        "  #print(mel_spec)\n",
        "  mel_spec = mel_spec.flatten()\n",
        "  #print(f\"Success! Mel shape: {mel_spec.shape}\")\n",
        "  example['audio'] = mel_spec"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "8KAxeCknp6Mk",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "8KAxeCknp6Mk",
        "outputId": "0cc05cfd-71c9-4aa6-c5db-cb1faef3e365"
      },
      "outputs": [
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "Converting Arrays to Spectograms: 100%|██████████| 4096/4096 [00:52<00:00, 77.49it/s] \n"
          ]
        }
      ],
      "source": [
        "# preprocess test audio arrays into mel spectograms\n",
        "\n",
        "for example in tqdm(testDS,total=totalTestRecs,desc=\"Converting Arrays to Spectograms\"):\n",
        "#for sample in trainingDS.take(1):\n",
        "  audio = example['audio']\n",
        "  audio_np = audio.numpy()\n",
        "\n",
        "  #print(f\"Audio shape: {audio_np.shape}, dtype: {audio_np.dtype}\")\n",
        "  mel_spec = librosa.feature.melspectrogram(\n",
        "      y=audio_np,\n",
        "      sr=16000,\n",
        "      n_fft=1024,\n",
        "      hop_length=256,\n",
        "      n_mels=64\n",
        "  )\n",
        "  mel_spec_db = librosa.power_to_db(mel_spec, ref=np.max)\n",
        "  #print(mel_spec_db)\n",
        " # Min-max normalization to [0, 1]\n",
        "  min_val = mel_spec_db.min()\n",
        "  max_val = mel_spec_db.max()\n",
        "  mel_spec= (mel_spec_db - min_val) / (max_val - min_val + 1e-6)\n",
        "  #print(f\"Success! Mel shape: {mel_spec.shape}\")\n",
        "  #print(mel_spec)\n",
        "  mel_spec = mel_spec.flatten()\n",
        "  #print(f\"Success! Mel shape: {mel_spec.shape}\")\n",
        "  example['audio'] = mel_spec"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "MlOBONtzTwVW",
      "metadata": {
        "id": "MlOBONtzTwVW"
      },
      "source": [
        "# PreProcessing Training Data in Batches\n",
        "\n",
        "### NOTE: Function creates files in /content/svmBatches/batchfeatures*, structure file directory accordingly or modify as needed"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 12,
      "id": "30692444",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "30692444",
        "outputId": "1371bffb-5396-46ec-ffc0-74314f3294de"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Batches to Parquet: 100%|██████████| 289/289 [17:00<00:00,  3.53s/it]\n",
            "Batches to Parquet: 100%|██████████| 289/289 [06:03<00:00,  1.26s/it]\n",
            "Batches to Parquet: 100%|██████████| 289/289 [16:28<00:00,  3.42s/it]\n",
            "Batches to Parquet: 100%|██████████| 289/289 [06:46<00:00,  1.40s/it]\n"
          ]
        }
      ],
      "source": [
        "def batch_parquet(parsedDataset, featnum, features=[], batch_size=1000, max_records=None):\n",
        "\n",
        "  batch_files = []\n",
        "  total_recs = 0\n",
        "  batch_idx = 0\n",
        "  # Convert dataset to batches\n",
        "  batched_dataset = parsedDataset.batch(batch_size)\n",
        "  n_batches = 289205 // batch_size\n",
        "  for batch in tqdm(batched_dataset,total=n_batches,desc=\"Batches to Parquet\"):\n",
        "    # Process each batch\n",
        "    batch_dict = {key: [] for key in features}\n",
        "\n",
        "    # Get batch size (might be smaller for the last batch)\n",
        "    current_batch_size = tf.shape(next(iter(batch.values())))[0].numpy()\n",
        "\n",
        "    # Extract features for each record in the batch\n",
        "    for i in range(current_batch_size):\n",
        "      for key in features:\n",
        "        # Handle different types of features\n",
        "        feature = batch[key][i]\n",
        "        if isinstance(feature, tf.Tensor):\n",
        "          value = feature.numpy()\n",
        "          # Convert bytes to string if applicable\n",
        "          if isinstance(value, bytes):\n",
        "            value = value.decode('utf-8')\n",
        "        else:\n",
        "            value = feature\n",
        "        batch_dict[key].append(value)\n",
        "\n",
        "    # Create DataFrame for this batch and append to list\n",
        "    df_batch = pd.DataFrame(batch_dict)\n",
        "\n",
        "    # Write to parquet\n",
        "    batch_file = f\"/content/svmBatches/bf{featnum}/batch_{batch_idx}.parquet\"\n",
        "    batch_idx += 1\n",
        "    df_batch.to_parquet(batch_file)\n",
        "    batch_files.append(batch_file)\n",
        "\n",
        "    # Update record count\n",
        "    total_recs += current_batch_size\n",
        "\n",
        "    # Check if we've reached the maximum number of records\n",
        "    if max_records is not None and total_recs >= max_records:\n",
        "        break\n",
        "  return\n",
        "# Usage:\n",
        "# NOTE: returns arent used, these need modification at some point\n",
        "# process features\n",
        "batch_parquet(trainingDS, 1, feat1, batch_size=1000,max_records=289205)\n",
        "batch_parquet(trainingDS, 2, feat2, batch_size=1000,max_records=289205)\n",
        "batch_parquet(trainingDS, 3, feat3, batch_size=1000,max_records=289205)\n",
        "\n",
        "# process target\n",
        "batch_parquet(trainingDS, 0, target, batch_size=1000,max_records=289205)\n"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "EHwTMSFqaOm8",
      "metadata": {
        "id": "EHwTMSFqaOm8"
      },
      "source": [
        "## Train SVM Model"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "45wnfWs0Z4ak",
      "metadata": {
        "id": "45wnfWs0Z4ak"
      },
      "source": [
        "## Check for Number of Cores available"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "vUXr3sq_EazZ",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "vUXr3sq_EazZ",
        "outputId": "e58dc947-fe39-4678-96dc-d086ad6e987b"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/usr/local/lib/python3.11/dist-packages/cudf/utils/_ptxcompiler.py:64: UserWarning: Error getting driver and runtime versions:\n",
            "\n",
            "stdout:\n",
            "\n",
            "\n",
            "\n",
            "stderr:\n",
            "\n",
            "Traceback (most recent call last):\n",
            "  File \"<string>\", line 4, in <module>\n",
            "  File \"/usr/local/lib/python3.11/dist-packages/numba_cuda/numba/cuda/cudadrv/driver.py\", line 314, in __getattr__\n",
            "    raise CudaSupportError(\"Error at driver init: \\n%s:\" %\n",
            "numba.cuda.cudadrv.error.CudaSupportError: Error at driver init: \n",
            "\n",
            "CUDA driver library cannot be found.\n",
            "If you are sure that a CUDA driver is installed,\n",
            "try setting environment variable NUMBA_CUDA_DRIVER\n",
            "with the file path of the CUDA driver shared library.\n",
            ":\n",
            "\n",
            "\n",
            "Not patching Numba\n",
            "  warnings.warn(msg, UserWarning)\n",
            "/usr/local/lib/python3.11/dist-packages/cudf/utils/gpu_utils.py:62: UserWarning: Failed to dlopen libcuda.so.1\n",
            "  warnings.warn(str(e))\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Available Cores: 8\n"
          ]
        }
      ],
      "source": [
        "\n",
        "import multiprocessing\n",
        "from sklearn.preprocessing import StandardScaler\n",
        "from cuml import MBSGDClassifier\n",
        "from sklearn.metrics import accuracy_score, precision_score, recall_score, f1_score\n",
        "from collections import Counter\n",
        "import joblib\n",
        "import glob\n",
        "import gc\n",
        "numCores = multiprocessing.cpu_count()\n",
        "print(f\"Available Cores: {numCores}\")"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "txmLjGZPZ_Qx",
      "metadata": {
        "id": "txmLjGZPZ_Qx"
      },
      "source": [
        "## Train Model in Batches"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "IxVXAZJTaGwH",
      "metadata": {
        "id": "IxVXAZJTaGwH"
      },
      "source": [
        "## Helper Functions for Training & FilePaths"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 14,
      "id": "sLK5OJPLP8eK",
      "metadata": {
        "id": "sLK5OJPLP8eK"
      },
      "outputs": [],
      "source": [
        "from sklearn.svm import SVC\n",
        "from sklearn.linear_model import SGDClassifier\n",
        "import glob\n",
        "import gc\n",
        "from sklearn.preprocessing import StandardScaler\n",
        "import ast\n",
        "from tqdm import tqdm\n",
        "import pandas as pd\n",
        "import joblib\n",
        "import librosa\n",
        "import librosa.display\n",
        "\n",
        "featuresDirs = [\n",
        "    \"/content/svmBatches/bf1\",\n",
        "    \"/content/svmBatches/bf2\",\n",
        "    \"/content/svmBatches/bf3\"\n",
        "]\n",
        "\n",
        "targetDir = \"/content/svmBatches/bf0\"\n",
        "\n",
        "def collect_all_classes(target_dir, num_batches):\n",
        "    print(\"Collecting unique classes...\")\n",
        "    all_classes = set()\n",
        "\n",
        "    for batch_idx in tqdm(range(num_batches), desc=\"Collecting unique classes\"):\n",
        "        try:\n",
        "            target_file = f\"{target_dir}/batch_{batch_idx}.parquet\"\n",
        "            y_batch = pd.read_parquet(target_file)\n",
        "            if y_batch.shape[1] == 1:\n",
        "                y_batch = y_batch.iloc[:, 0].values\n",
        "\n",
        "            # Add all unique classes from this batch\n",
        "            batch_classes = np.unique(y_batch)\n",
        "            all_classes.update(batch_classes)\n",
        "\n",
        "        except Exception as e:\n",
        "            print(f\"Error loading target {batch_idx}: {e}\")\n",
        "            # If we hit a consistent error, we might have reached the end\n",
        "            if \"No such file or directory\" in str(e):\n",
        "                break\n",
        "\n",
        "    all_classes = np.array(sorted(list(all_classes)))\n",
        "    print(f\"Found {len(all_classes)} unique classes in the target\")\n",
        "    return all_classes\n",
        "\n",
        "def load_batch_from_multiple_dirs(batch_idx, feature_dirs, target_dir):\n",
        "  \"\"\"Load features from multiple directories and combine them\"\"\"\n",
        "  feature_arrays = []\n",
        "  audio_data = None  # Separate variable to store audio data\n",
        "\n",
        "  # Load features from each directory\n",
        "  for feature_dir in feature_dirs:\n",
        "    feature_file = f\"{feature_dir}/batch_{batch_idx}.parquet\"\n",
        "    try:\n",
        "      if os.path.exists(feature_file):\n",
        "        df = pd.read_parquet(feature_file)\n",
        "        # Check if this is the audio feature\n",
        "      if 'audio' in df.columns:\n",
        "        # Reshape audio data to 2D\n",
        "        audio_data = df['audio'].apply(lambda x: np.array(x).reshape(1, -1)).values\n",
        "        audio_data = np.concatenate(audio_data, axis=0)\n",
        "      else:\n",
        "        # Append other features to list\n",
        "        feature_arrays.append(df.values)\n",
        "      del df\n",
        "      gc.collect()\n",
        "\n",
        "    except Exception as e:\n",
        "      print(f\"Error loading {feature_file}: {e}\")\n",
        "      #return None, None\n",
        "\n",
        "    # Combine features horizontally, excluding audio\n",
        "\n",
        "    try:\n",
        "        X_batch_other_features = np.concatenate(feature_arrays, axis=1)\n",
        "    except Exception as e:\n",
        "        print(f\"Error concatenating features for batch {batch_idx}: {e}\")\n",
        "\n",
        "    # Load target\n",
        "    target_file = f\"{target_dir}/batch_{batch_idx}.parquet\"\n",
        "    try:\n",
        "      y_batch = pd.read_parquet(target_file)\n",
        "      if y_batch.shape[1] == 1:\n",
        "        y_batch = y_batch.iloc[:, 0].values\n",
        "      else:\n",
        "        y_batch = y_batch.values\n",
        "    except Exception as e:\n",
        "        print(f\"Error loading target for batch {batch_idx}: {e}\")\n",
        "\n",
        "    # Combine other features and audio\n",
        "    if audio_data is not None:\n",
        "      X_batch = np.concatenate([X_batch_other_features, audio_data], axis=1)\n",
        "    else:\n",
        "      X_batch = X_batch_other_features\n",
        "\n",
        "    del X_batch_other_features\n",
        "    gc.collect()\n",
        "  return X_batch, y_batch"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "nF7rD0Uiexcb",
      "metadata": {
        "id": "nF7rD0Uiexcb"
      },
      "source": [
        "# Train Model"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 23,
      "id": "svYYw1qqt4tE",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "svYYw1qqt4tE",
        "outputId": "33b25dae-25e2-46f3-84d2-eda7fbb4a026"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Training model...\n",
            "Collecting unique classes...\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Collecting unique classes: 100%|██████████| 289/289 [00:00<00:00, 350.19it/s]\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Found 11 unique classes in the target\n",
            "Fitting scaler incrementally...\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Fitting scaler incrementally: 100%|██████████| 29/29 [03:31<00:00,  7.31s/it]\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Scaler fitted!\n",
            "Computing class weights...\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Collecting targets for class weights: 100%|██████████| 289/289 [24:00<00:00,  4.98s/it]\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Class weights calculated!\n",
            "\n",
            "=== Epoch 1/2 ===\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Epoch 1, Chunk 0: 100%|██████████| 10/10 [00:44<00:00,  4.44s/it]\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Training on 10000 samples...\n",
            "Fitting time: 6.719 seconds\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Epoch 1, Chunk 1: 100%|██████████| 10/10 [00:45<00:00,  4.57s/it]\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Training on 10000 samples...\n",
            "Fitting time: 3.826 seconds\n",
            "Test accuracy (on previous chunk): 0.4495\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Epoch 1, Chunk 2: 100%|██████████| 10/10 [00:43<00:00,  4.37s/it]\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Training on 10000 samples...\n",
            "Fitting time: 3.639 seconds\n",
            "Test accuracy (on previous chunk): 0.4945\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Epoch 1, Chunk 3: 100%|██████████| 10/10 [00:42<00:00,  4.30s/it]\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Training on 10000 samples...\n",
            "Fitting time: 3.808 seconds\n",
            "Test accuracy (on previous chunk): 0.3685\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Epoch 1, Chunk 4: 100%|██████████| 10/10 [00:48<00:00,  4.87s/it]\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Training on 10000 samples...\n",
            "Fitting time: 4.233 seconds\n",
            "Test accuracy (on previous chunk): 0.2830\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Epoch 1, Chunk 5: 100%|██████████| 10/10 [01:09<00:00,  6.94s/it]\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Training on 10000 samples...\n",
            "Fitting time: 7.587 seconds\n",
            "Test accuracy (on previous chunk): 0.3945\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Epoch 1, Chunk 6: 100%|██████████| 10/10 [00:49<00:00,  4.99s/it]\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Training on 10000 samples...\n",
            "Fitting time: 3.988 seconds\n",
            "Test accuracy (on previous chunk): 0.3510\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Epoch 1, Chunk 7: 100%|██████████| 10/10 [00:48<00:00,  4.88s/it]\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Training on 10000 samples...\n",
            "Fitting time: 3.625 seconds\n",
            "Test accuracy (on previous chunk): 0.4410\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Epoch 1, Chunk 8: 100%|██████████| 10/10 [00:46<00:00,  4.66s/it]\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Training on 10000 samples...\n",
            "Fitting time: 3.717 seconds\n",
            "Test accuracy (on previous chunk): 0.2900\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Epoch 1, Chunk 9: 100%|██████████| 10/10 [00:44<00:00,  4.47s/it]\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Training on 10000 samples...\n",
            "Fitting time: 3.947 seconds\n",
            "Test accuracy (on previous chunk): 0.1450\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Epoch 1, Chunk 10: 100%|██████████| 10/10 [00:44<00:00,  4.48s/it]\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Training on 10000 samples...\n",
            "Fitting time: 3.823 seconds\n",
            "Test accuracy (on previous chunk): 0.3575\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Epoch 1, Chunk 11: 100%|██████████| 10/10 [00:48<00:00,  4.86s/it]\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Training on 10000 samples...\n",
            "Fitting time: 4.328 seconds\n",
            "Test accuracy (on previous chunk): 0.3010\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Epoch 1, Chunk 12: 100%|██████████| 10/10 [01:05<00:00,  6.60s/it]\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Training on 10000 samples...\n",
            "Fitting time: 4.331 seconds\n",
            "Test accuracy (on previous chunk): 0.2215\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Epoch 1, Chunk 13: 100%|██████████| 10/10 [00:47<00:00,  4.73s/it]\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Training on 10000 samples...\n",
            "Fitting time: 4.145 seconds\n",
            "Test accuracy (on previous chunk): 0.2620\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Epoch 1, Chunk 14: 100%|██████████| 10/10 [00:46<00:00,  4.64s/it]\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Training on 10000 samples...\n",
            "Fitting time: 3.872 seconds\n",
            "Test accuracy (on previous chunk): 0.2835\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Epoch 1, Chunk 15: 100%|██████████| 10/10 [01:19<00:00,  7.95s/it]\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Training on 10000 samples...\n",
            "Fitting time: 3.793 seconds\n",
            "Test accuracy (on previous chunk): 0.1745\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Epoch 1, Chunk 16: 100%|██████████| 10/10 [00:49<00:00,  4.90s/it]\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Training on 10000 samples...\n",
            "Fitting time: 3.742 seconds\n",
            "Test accuracy (on previous chunk): 0.2425\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Epoch 1, Chunk 17: 100%|██████████| 10/10 [00:47<00:00,  4.79s/it]\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Training on 10000 samples...\n",
            "Fitting time: 3.924 seconds\n",
            "Test accuracy (on previous chunk): 0.2345\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Epoch 1, Chunk 18: 100%|██████████| 10/10 [00:46<00:00,  4.61s/it]\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Training on 10000 samples...\n",
            "Fitting time: 3.704 seconds\n",
            "Test accuracy (on previous chunk): 0.2075\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Epoch 1, Chunk 19: 100%|██████████| 10/10 [00:43<00:00,  4.30s/it]\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Training on 10000 samples...\n",
            "Fitting time: 3.678 seconds\n",
            "Test accuracy (on previous chunk): 0.2965\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Epoch 1, Chunk 20: 100%|██████████| 10/10 [00:45<00:00,  4.57s/it]\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Training on 10000 samples...\n",
            "Fitting time: 4.093 seconds\n",
            "Test accuracy (on previous chunk): 0.3035\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Epoch 1, Chunk 21: 100%|██████████| 10/10 [00:47<00:00,  4.77s/it]\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Training on 10000 samples...\n",
            "Fitting time: 4.186 seconds\n",
            "Test accuracy (on previous chunk): 0.2355\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Epoch 1, Chunk 22: 100%|██████████| 10/10 [00:46<00:00,  4.64s/it]\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Training on 10000 samples...\n",
            "Fitting time: 3.794 seconds\n",
            "Test accuracy (on previous chunk): 0.1580\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Epoch 1, Chunk 23: 100%|██████████| 10/10 [00:47<00:00,  4.70s/it]\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Training on 10000 samples...\n",
            "Fitting time: 3.820 seconds\n",
            "Test accuracy (on previous chunk): 0.1820\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Epoch 1, Chunk 24: 100%|██████████| 10/10 [00:41<00:00,  4.16s/it]\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Training on 10000 samples...\n",
            "Fitting time: 3.796 seconds\n",
            "Test accuracy (on previous chunk): 0.2940\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Epoch 1, Chunk 25: 100%|██████████| 10/10 [00:47<00:00,  4.78s/it]\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Training on 10000 samples...\n",
            "Fitting time: 3.901 seconds\n",
            "Test accuracy (on previous chunk): 0.1520\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Epoch 1, Chunk 26: 100%|██████████| 10/10 [00:50<00:00,  5.01s/it]\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Training on 10000 samples...\n",
            "Fitting time: 4.047 seconds\n",
            "Test accuracy (on previous chunk): 0.4250\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Epoch 1, Chunk 27: 100%|██████████| 10/10 [00:51<00:00,  5.11s/it]\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Training on 10000 samples...\n",
            "Fitting time: 3.845 seconds\n",
            "Test accuracy (on previous chunk): 0.2065\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Epoch 1, Chunk 28: 100%|██████████| 9/9 [00:43<00:00,  4.89s/it]\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Training on 9000 samples...\n",
            "Fitting time: 3.575 seconds\n",
            "Test accuracy (on previous chunk): 0.1760\n",
            "\n",
            "Epoch 1 complete\n",
            "Samples processed: 289000\n",
            "Mean accuracy: 0.2832\n",
            "Recent accuracies: [0.294, 0.152, 0.425, 0.2065, 0.176]\n",
            "Time: 31.46 minutes\n",
            "\n",
            "=== Epoch 2/2 ===\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Epoch 2, Chunk 0: 100%|██████████| 10/10 [00:47<00:00,  4.73s/it]\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Training on 10000 samples...\n",
            "Fitting time: 3.986 seconds\n",
            "Test accuracy (on previous chunk): 0.2930\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Epoch 2, Chunk 1: 100%|██████████| 10/10 [00:46<00:00,  4.64s/it]\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Training on 10000 samples...\n",
            "Fitting time: 3.866 seconds\n",
            "Test accuracy (on previous chunk): 0.2130\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Epoch 2, Chunk 2: 100%|██████████| 10/10 [00:46<00:00,  4.64s/it]\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Training on 10000 samples...\n",
            "Fitting time: 4.044 seconds\n",
            "Test accuracy (on previous chunk): 0.2520\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Epoch 2, Chunk 3: 100%|██████████| 10/10 [00:43<00:00,  4.35s/it]\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Training on 10000 samples...\n",
            "Fitting time: 3.836 seconds\n",
            "Test accuracy (on previous chunk): 0.1680\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Epoch 2, Chunk 4: 100%|██████████| 10/10 [00:46<00:00,  4.66s/it]\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Training on 10000 samples...\n",
            "Fitting time: 3.654 seconds\n",
            "Test accuracy (on previous chunk): 0.2815\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Epoch 2, Chunk 5: 100%|██████████| 10/10 [00:46<00:00,  4.64s/it]\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Training on 10000 samples...\n",
            "Fitting time: 3.672 seconds\n",
            "Test accuracy (on previous chunk): 0.1865\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Epoch 2, Chunk 6: 100%|██████████| 10/10 [00:45<00:00,  4.52s/it]\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Training on 10000 samples...\n",
            "Fitting time: 3.867 seconds\n",
            "Test accuracy (on previous chunk): 0.3510\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Epoch 2, Chunk 7: 100%|██████████| 10/10 [00:53<00:00,  5.35s/it]\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Training on 10000 samples...\n",
            "Fitting time: 3.771 seconds\n",
            "Test accuracy (on previous chunk): 0.2725\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Epoch 2, Chunk 8: 100%|██████████| 10/10 [00:44<00:00,  4.45s/it]\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Training on 10000 samples...\n",
            "Fitting time: 3.761 seconds\n",
            "Test accuracy (on previous chunk): 0.2800\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Epoch 2, Chunk 9: 100%|██████████| 10/10 [00:43<00:00,  4.32s/it]\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Training on 10000 samples...\n",
            "Fitting time: 3.687 seconds\n",
            "Test accuracy (on previous chunk): 0.1855\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Epoch 2, Chunk 10: 100%|██████████| 10/10 [00:54<00:00,  5.46s/it]\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Training on 10000 samples...\n",
            "Fitting time: 10.391 seconds\n",
            "Test accuracy (on previous chunk): 0.1535\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Epoch 2, Chunk 11: 100%|██████████| 10/10 [00:43<00:00,  4.34s/it]\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Training on 10000 samples...\n",
            "Fitting time: 3.620 seconds\n",
            "Test accuracy (on previous chunk): 0.3105\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Epoch 2, Chunk 12: 100%|██████████| 10/10 [00:44<00:00,  4.45s/it]\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Training on 10000 samples...\n",
            "Fitting time: 3.812 seconds\n",
            "Test accuracy (on previous chunk): 0.2715\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Epoch 2, Chunk 13: 100%|██████████| 10/10 [00:46<00:00,  4.68s/it]\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Training on 10000 samples...\n",
            "Fitting time: 3.848 seconds\n",
            "Test accuracy (on previous chunk): 0.3745\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Epoch 2, Chunk 14: 100%|██████████| 10/10 [00:43<00:00,  4.32s/it]\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Training on 10000 samples...\n",
            "Fitting time: 3.706 seconds\n",
            "Test accuracy (on previous chunk): 0.1430\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Epoch 2, Chunk 15: 100%|██████████| 10/10 [00:42<00:00,  4.26s/it]\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Training on 10000 samples...\n",
            "Fitting time: 3.806 seconds\n",
            "Test accuracy (on previous chunk): 0.3105\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Epoch 2, Chunk 16: 100%|██████████| 10/10 [00:45<00:00,  4.53s/it]\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Training on 10000 samples...\n",
            "Fitting time: 3.627 seconds\n",
            "Test accuracy (on previous chunk): 0.2895\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Epoch 2, Chunk 17: 100%|██████████| 10/10 [00:59<00:00,  5.93s/it]\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Training on 10000 samples...\n",
            "Fitting time: 3.842 seconds\n",
            "Test accuracy (on previous chunk): 0.2335\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Epoch 2, Chunk 18: 100%|██████████| 10/10 [00:45<00:00,  4.52s/it]\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Training on 10000 samples...\n",
            "Fitting time: 3.719 seconds\n",
            "Test accuracy (on previous chunk): 0.1970\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Epoch 2, Chunk 19: 100%|██████████| 10/10 [00:46<00:00,  4.63s/it]\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Training on 10000 samples...\n",
            "Fitting time: 3.666 seconds\n",
            "Test accuracy (on previous chunk): 0.1895\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Epoch 2, Chunk 20: 100%|██████████| 10/10 [00:44<00:00,  4.47s/it]\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Training on 10000 samples...\n",
            "Fitting time: 3.747 seconds\n",
            "Test accuracy (on previous chunk): 0.1380\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Epoch 2, Chunk 21: 100%|██████████| 10/10 [00:45<00:00,  4.52s/it]\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Training on 10000 samples...\n",
            "Fitting time: 3.788 seconds\n",
            "Test accuracy (on previous chunk): 0.2340\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Epoch 2, Chunk 22: 100%|██████████| 10/10 [00:47<00:00,  4.73s/it]\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Training on 10000 samples...\n",
            "Fitting time: 3.830 seconds\n",
            "Test accuracy (on previous chunk): 0.1715\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Epoch 2, Chunk 23: 100%|██████████| 10/10 [00:47<00:00,  4.75s/it]\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Training on 10000 samples...\n",
            "Fitting time: 4.240 seconds\n",
            "Test accuracy (on previous chunk): 0.2255\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Epoch 2, Chunk 24: 100%|██████████| 10/10 [00:47<00:00,  4.75s/it]\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Training on 10000 samples...\n",
            "Fitting time: 3.650 seconds\n",
            "Test accuracy (on previous chunk): 0.1710\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Epoch 2, Chunk 25: 100%|██████████| 10/10 [00:43<00:00,  4.39s/it]\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Training on 10000 samples...\n",
            "Fitting time: 3.977 seconds\n",
            "Test accuracy (on previous chunk): 0.1680\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Epoch 2, Chunk 26: 100%|██████████| 10/10 [00:44<00:00,  4.46s/it]\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Training on 10000 samples...\n",
            "Fitting time: 6.321 seconds\n",
            "Test accuracy (on previous chunk): 0.3295\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Epoch 2, Chunk 27: 100%|██████████| 10/10 [00:52<00:00,  5.23s/it]\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Training on 10000 samples...\n",
            "Fitting time: 3.856 seconds\n",
            "Test accuracy (on previous chunk): 0.1725\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Epoch 2, Chunk 28: 100%|██████████| 9/9 [00:46<00:00,  5.15s/it]\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Training on 9000 samples...\n",
            "Fitting time: 3.284 seconds\n",
            "Test accuracy (on previous chunk): 0.2100\n",
            "\n",
            "Epoch 2 complete\n",
            "Samples processed: 578000\n",
            "Mean accuracy: 0.2337\n",
            "Recent accuracies: [0.171, 0.168, 0.3295, 0.1725, 0.21]\n",
            "Time: 30.11 minutes\n",
            "\n",
            "Training complete\n",
            "Final accuracies: [0.1895, 0.138, 0.234, 0.1715, 0.2255, 0.171, 0.168, 0.3295, 0.1725, 0.21]\n",
            "Total Model Training Time: 89.21959587732951 mins\n",
            "New model saved!\n"
          ]
        }
      ],
      "source": [
        "from sklearn.linear_model import SGDClassifier\n",
        "from sklearn.utils.class_weight import compute_class_weight\n",
        "import os\n",
        "\n",
        "scaler = StandardScaler()\n",
        "\n",
        "def train_model(features_dir, target_dir, num_batches, batch_size=10):\n",
        "# Collect all classes\n",
        "    all_classes = collect_all_classes(target_dir, num_batches)\n",
        "\n",
        "      # Fit scaler incrementally\n",
        "    print(\"Fitting scaler incrementally...\")\n",
        "    n_scaler_batches = min(30, num_batches)  # Use fewer batches for scaler\n",
        "    scaler_batches = np.random.choice(num_batches, size=n_scaler_batches, replace=False)\n",
        "\n",
        "    # First batch to initialize scaler\n",
        "    first_batch_idx = scaler_batches[0]\n",
        "    X_first, _ = load_batch_from_multiple_dirs(first_batch_idx, features_dir, target_dir)\n",
        "    if X_first is not None:\n",
        "        # Sample subset if batch is large\n",
        "        if X_first.shape[0] > 5000:\n",
        "            sample_indices = np.random.choice(X_first.shape[0], 5000, replace=False)\n",
        "            X_first = X_first[sample_indices]\n",
        "        scaler.partial_fit(X_first)\n",
        "        del X_first\n",
        "        gc.collect()\n",
        "    # Fit on remaining batches\n",
        "    for batch_idx in tqdm(scaler_batches[1:], desc=\"Fitting scaler incrementally\"):\n",
        "        X_batch, _ = load_batch_from_multiple_dirs(batch_idx, features_dir, target_dir)\n",
        "        if X_batch is not None:\n",
        "            # Sample subset if batch is large\n",
        "            if X_batch.shape[0] > 5000:\n",
        "                sample_indices = np.random.choice(X_batch.shape[0], 5000, replace=False)\n",
        "                X_batch = X_batch[sample_indices]\n",
        "            scaler.partial_fit(X_batch)\n",
        "            del X_batch\n",
        "            gc.collect()\n",
        "\n",
        "    print(\"Scaler fitted!\")\n",
        "    # Collect targets for class weights\n",
        "    print(\"Computing class weights...\")\n",
        "    train_targets = []\n",
        "    for batch_idx in tqdm(range(num_batches), desc=\"Collecting targets for class weights\"):\n",
        "        _, y_batch = load_batch_from_multiple_dirs(batch_idx, features_dir, target_dir)\n",
        "        if y_batch is not None:\n",
        "            train_targets.extend(y_batch)\n",
        "\n",
        "    train_targets = np.array(train_targets)\n",
        "    class_weights = compute_class_weight('balanced', classes=all_classes, y=train_targets)\n",
        "    class_weights_dict = dict(zip(all_classes, class_weights))\n",
        "    class_weights_dict[2] *= 1.5  # Boost weight for class 2\n",
        "    class_weights_dict[3] *= 1.5  # Boost weight for class 3\n",
        "    class_weights_dict[10] *= 5.0  # Significantly boost class 10\n",
        "    print(\"Class weights calculated!\")\n",
        "\n",
        "    # Initialize model\n",
        "    model = SGDClassifier(\n",
        "        loss='modified_huber',\n",
        "        learning_rate='optimal',\n",
        "        eta0=0.001,\n",
        "        alpha=0.1,\n",
        "        l1_ratio=0.2,\n",
        "        fit_intercept=True,\n",
        "        tol=1e-4,\n",
        "        class_weight=class_weights_dict,\n",
        "        random_state=42,\n",
        "        n_jobs=8,\n",
        "        warm_start=True  # warm start for partial_fit\n",
        "    )\n",
        "\n",
        "    # Training\n",
        "    train_accuracies = []\n",
        "    n_samples_processed = 0\n",
        "    n_epochs = 2\n",
        "\n",
        "    # Store the last chunk for testing\n",
        "    previous_X_test = None\n",
        "    previous_y_test = None\n",
        "\n",
        "    for epoch in range(n_epochs):\n",
        "        epoch_time = time.time()\n",
        "        print(f\"\\n=== Epoch {epoch + 1}/{n_epochs} ===\")\n",
        "\n",
        "        all_batch_indices = list(range(num_batches))\n",
        "\n",
        "        # Shuffle multiple times and interleave\n",
        "        shuffled_sets = []\n",
        "        for _ in range(3):\n",
        "            shuffled = all_batch_indices.copy()\n",
        "            np.random.shuffle(shuffled)\n",
        "            shuffled_sets.append(shuffled)\n",
        "\n",
        "        # Interleave the shuffles\n",
        "        batch_indices = []\n",
        "        for i in range(num_batches):\n",
        "            batch_indices.append(shuffled_sets[i % 3][i // 3])\n",
        "        epoch_accuracies = []\n",
        "\n",
        "        for chunk_start in range(0, num_batches, batch_size):\n",
        "            chunk_end = min(chunk_start + batch_size, num_batches)\n",
        "            chunk_indices = batch_indices[chunk_start:chunk_end]\n",
        "\n",
        "            X_chunk_list = []\n",
        "            y_chunk_list = []\n",
        "\n",
        "            # Load batches in this chunk\n",
        "            for batch_idx in tqdm(chunk_indices, desc=f\"Epoch {epoch+1}, Chunk {chunk_start//batch_size}\"):\n",
        "                X_batch, y_batch = load_batch_from_multiple_dirs(batch_idx, features_dir, target_dir)\n",
        "\n",
        "                if X_batch is not None and y_batch is not None:\n",
        "                    X_chunk_list.append(X_batch)\n",
        "                    y_chunk_list.append(y_batch)\n",
        "\n",
        "            if X_chunk_list:\n",
        "                # Stack and scale\n",
        "                X_chunk = np.vstack(X_chunk_list)\n",
        "                y_chunk = np.concatenate(y_chunk_list)\n",
        "                X_chunk_scaled = scaler.transform(X_chunk)\n",
        "\n",
        "                # Train\n",
        "                print(f\"Training on {X_chunk.shape[0]} samples...\")\n",
        "                start_time = time.time()\n",
        "\n",
        "                if n_samples_processed == 0:\n",
        "                    model.partial_fit(X_chunk_scaled, y_chunk, classes=all_classes)\n",
        "                else:\n",
        "                    model.partial_fit(X_chunk_scaled, y_chunk)\n",
        "\n",
        "                fit_time = time.time() - start_time\n",
        "                print(f\"Fitting time: {fit_time:.3f} seconds\")\n",
        "                n_samples_processed += X_chunk.shape[0]\n",
        "\n",
        "                # Test on previous chunk (if available) and test every 5 chunks\n",
        "                if previous_X_test is not None and chunk_start % 5 == 0:\n",
        "                    test_pred = model.predict(previous_X_test)\n",
        "                    test_acc = accuracy_score(previous_y_test, test_pred)\n",
        "                    train_accuracies.append(test_acc)\n",
        "                    epoch_accuracies.append(test_acc)\n",
        "                    print(f\"Test accuracy (on previous chunk): {test_acc:.4f}\")\n",
        "\n",
        "                # Save current chunk for next iteration's testing\n",
        "                test_size = min(2000, X_chunk_scaled.shape[0])\n",
        "                test_indices = np.random.choice(X_chunk_scaled.shape[0], test_size, replace=False)\n",
        "                previous_X_test = X_chunk_scaled[test_indices].copy()\n",
        "                previous_y_test = y_chunk[test_indices].copy()\n",
        "\n",
        "                del X_chunk, X_chunk_scaled, y_chunk\n",
        "                gc.collect()\n",
        "\n",
        "        # Epoch summary\n",
        "        if epoch_accuracies:\n",
        "            epoch_mean_acc = np.mean(epoch_accuracies)\n",
        "            print(f\"\\nEpoch {epoch + 1} complete\")\n",
        "            print(f\"Samples processed: {n_samples_processed}\")\n",
        "            print(f\"Mean accuracy: {epoch_mean_acc:.4f}\")\n",
        "            print(f\"Recent accuracies: {train_accuracies[-5:]}\")\n",
        "        else:\n",
        "            print(f\"\\nEpoch {epoch + 1} complete\")\n",
        "            print(f\"Samples processed: {n_samples_processed}\")\n",
        "\n",
        "        print(f\"Time: {(time.time() - epoch_time)/60:.2f} minutes\")\n",
        "\n",
        "    print(\"\\nTraining complete\")\n",
        "    print(f\"Final accuracies: {train_accuracies[-10:]}\")\n",
        "\n",
        "    return model, scaler\n",
        "\n",
        "print(\"Training model...\")\n",
        "begTime = time.time()\n",
        "model_new = train_model(featuresDirs, targetDir, num_batches=289)\n",
        "endTime = time.time() - begTime\n",
        "print(f\"Total Model Training Time: {float(endTime/60)} mins\")\n",
        "# Save the model and the scaler\n",
        "joblib.dump(model_new, '/content/drive/MyDrive/svm3_with_scaler.pkl')\n",
        "joblib.dump(scaler, '/content/drive/MyDrive/svm3_scaler.pkl')\n",
        "print(\"New model saved!\")"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "30pzWQjYcqwt",
      "metadata": {
        "id": "30pzWQjYcqwt"
      },
      "source": [
        "# PreProcessing Validation Data in Batches"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 24,
      "id": "0Dw74mATIv1U",
      "metadata": {
        "id": "0Dw74mATIv1U",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "a1d13df4-98c7-42a8-8af6-6532063e5d38"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Batches to Parquet: 100%|██████████| 12/12 [00:34<00:00,  2.85s/it]\n",
            "Batches to Parquet: 100%|██████████| 12/12 [00:09<00:00,  1.20it/s]\n",
            "Batches to Parquet: 100%|██████████| 12/12 [00:38<00:00,  3.20s/it]\n",
            "Batches to Parquet: 100%|██████████| 12/12 [00:06<00:00,  1.97it/s]\n"
          ]
        }
      ],
      "source": [
        "featureValidDirs = [\n",
        "    \"/content/svmBatches/bv1\",\n",
        "    \"/content/svmBatches/bv2\",\n",
        "    \"/content/svmBatches/bv3\"\n",
        "]\n",
        "\n",
        "targetValidDir = \"/content/svmBatches/bv0\"\n",
        "\n",
        "def batch_parquet(parsedDataset, featnum, features=[], batch_size=1000, max_records=None):\n",
        "\n",
        "  batch_files = []\n",
        "  total_recs = 0\n",
        "  batch_idx = 0\n",
        "  # Convert dataset to batches\n",
        "  batched_dataset = parsedDataset.batch(batch_size)\n",
        "  n_batches = totalValidRecs // batch_size\n",
        "  for batch in tqdm(batched_dataset,total=n_batches,desc=\"Batches to Parquet\"):\n",
        "    # Process each batch\n",
        "    batch_dict = {key: [] for key in features}\n",
        "\n",
        "    # Get batch size (might be smaller for the last batch)\n",
        "    current_batch_size = tf.shape(next(iter(batch.values())))[0].numpy()\n",
        "\n",
        "    # Extract features for each record in the batch\n",
        "    for i in range(current_batch_size):\n",
        "      for key in features:\n",
        "        # Handle different types of features\n",
        "        feature = batch[key][i]\n",
        "        if isinstance(feature, tf.Tensor):\n",
        "          value = feature.numpy()\n",
        "          # Convert bytes to string if applicable\n",
        "          if isinstance(value, bytes):\n",
        "            value = value.decode('utf-8')\n",
        "        else:\n",
        "            value = feature\n",
        "        batch_dict[key].append(value)\n",
        "\n",
        "    # Create DataFrame for this batch and append to list\n",
        "    df_batch = pd.DataFrame(batch_dict)\n",
        "\n",
        "    # Write to parquet\n",
        "    batch_file = f\"/content/svmBatches/bv{featnum}/batch_{batch_idx}.parquet\"\n",
        "    batch_idx += 1\n",
        "    df_batch.to_parquet(batch_file)\n",
        "    batch_files.append(batch_file)\n",
        "\n",
        "    # Update record count\n",
        "    total_recs += current_batch_size\n",
        "\n",
        "    # Check if we've reached the maximum number of records\n",
        "    if max_records is not None and total_recs >= max_records:\n",
        "        break\n",
        "  return\n",
        "# Usage:\n",
        "# NOTE: returns arent used, these need modification at some point\n",
        "# process features\n",
        "batch_parquet(validDS, 1, feat1, batch_size=1000,max_records=totalValidRecs)\n",
        "batch_parquet(validDS, 2, feat2, batch_size=1000,max_records=totalValidRecs)\n",
        "batch_parquet(validDS, 3, feat3, batch_size=1000,max_records=totalValidRecs)\n",
        "\n",
        "# process target\n",
        "batch_parquet(validDS, 0, target, batch_size=1000,max_records=totalValidRecs)\n"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "xPvOly7DcyQd",
      "metadata": {
        "id": "xPvOly7DcyQd"
      },
      "source": [
        "## Validate SVM Model"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 25,
      "id": "id9VHlKZFUG-",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "id9VHlKZFUG-",
        "outputId": "9e849ed8-eabc-4e53-ca76-68920ec5bb20"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\n",
            "=== Starting Validation ===\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Validation Chunk 0: 100%|██████████| 12/12 [00:48<00:00,  4.01s/it]\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Predicting on 12000 samples...\n",
            "Chunk accuracy: 0.1134\n",
            "\n",
            "=== Validation Complete ===\n",
            "Total samples processed: 12000\n",
            "Overall accuracy: 0.1134\n",
            "F1-score (weighted): 0.1265\n",
            "F1-score (macro): 0.1007\n",
            "F1-score (micro): 0.1134\n",
            "Precision (weighted): 0.2752\n",
            "Precision (macro): 0.2369\n",
            "\n",
            "Per-class F1 scores:\n",
            "  Class 0: 0.2595\n",
            "  Class 1: 0.0954\n",
            "  Class 2: 0.0310\n",
            "  Class 3: 0.0269\n",
            "  Class 4: 0.0120\n",
            "  Class 5: 0.1796\n",
            "  Class 6: 0.2387\n",
            "  Class 7: 0.0741\n",
            "  Class 8: 0.1678\n",
            "  Class 10: 0.0000\n"
          ]
        }
      ],
      "source": [
        "from sklearn.metrics import (accuracy_score, f1_score, precision_score, recall_score,\n",
        "                            classification_report, confusion_matrix, matthews_corrcoef,\n",
        "                            roc_auc_score, roc_curve, auc)\n",
        "import os\n",
        "#model = joblib.load('/content/drive/MyDrive/svm2_with_scaler.pkl')\n",
        "\n",
        "featureValidDirs = [\n",
        "    \"/content/svmBatches/bv1\",\n",
        "    \"/content/svmBatches/bv2\",\n",
        "    \"/content/svmBatches/bv3\"\n",
        "]\n",
        "\n",
        "targetValidDir = \"/content/svmBatches/bv0\"\n",
        "\n",
        "def validate_model(model_path,scaler_path, features_dir, target_dir, num_batches, batch_size=12):\n",
        "\n",
        "    model = joblib.load(model_path)\n",
        "    # Check if it's a tuple and extract the model\n",
        "    if isinstance(model, tuple):\n",
        "        # Assume the model is the first element\n",
        "        model = model[0]\n",
        "    else:\n",
        "        model = model\n",
        "    scaler = joblib.load(scaler_path)\n",
        "    # Track validation progress\n",
        "    all_predictions = []\n",
        "    all_true_labels = []\n",
        "    n_samples_processed = 0\n",
        "\n",
        "    #is_scaler_fitted = False\n",
        "    print(f\"\\n=== Starting Validation ===\")\n",
        "\n",
        "    for chunk_start in range(0, num_batches, batch_size):\n",
        "        chunk_end = min(chunk_start + batch_size, num_batches)\n",
        "        chunk_indices = range(chunk_start, chunk_end)\n",
        "\n",
        "        X_chunk_list = []\n",
        "        y_chunk_list = []\n",
        "\n",
        "        # Load batches in this chunk\n",
        "        for batch_idx in tqdm(chunk_indices, desc=f\"Validation Chunk {chunk_start//batch_size}\"):\n",
        "            X_batch, y_batch = load_batch_from_multiple_dirs(batch_idx, features_dir, target_dir)\n",
        "\n",
        "            if X_batch is not None and y_batch is not None:\n",
        "                X_chunk_list.append(X_batch)\n",
        "                y_chunk_list.append(y_batch)\n",
        "        gc.collect()\n",
        "\n",
        "        if X_chunk_list:\n",
        "            # Stack all batches in chunk\n",
        "            X_chunk = np.vstack(X_chunk_list)\n",
        "            y_chunk = np.concatenate(y_chunk_list)\n",
        "\n",
        "            # Scale features using the provided scaler\n",
        "            X_chunk_scaled = scaler.transform(X_chunk)\n",
        "\n",
        "            # Make predictions\n",
        "            print(f\"Predicting on {X_chunk.shape[0]} samples...\")\n",
        "            chunk_predictions = model.predict(X_chunk_scaled)\n",
        "\n",
        "            # Collect results\n",
        "            all_predictions.extend(chunk_predictions)\n",
        "            all_true_labels.extend(y_chunk)\n",
        "            n_samples_processed += X_chunk.shape[0]\n",
        "\n",
        "            # Calculate chunk accuracy for progress monitoring\n",
        "            chunk_acc = accuracy_score(y_chunk, chunk_predictions)\n",
        "            print(f\"Chunk accuracy: {chunk_acc:.4f}\")\n",
        "\n",
        "            del X_chunk, X_chunk_scaled, y_chunk, chunk_predictions\n",
        "            gc.collect()\n",
        "\n",
        "    # Convert to numpy arrays for final metrics\n",
        "    all_predictions = np.array(all_predictions)\n",
        "    all_true_labels = np.array(all_true_labels)\n",
        "    metrics = {}\n",
        "\n",
        "    # Basic metrics\n",
        "    metrics['accuracy'] = accuracy_score(all_true_labels, all_predictions)\n",
        "\n",
        "    # F1 scores - multiple averaging methods\n",
        "    metrics['f1_weighted'] = f1_score(all_true_labels, all_predictions, average='weighted')\n",
        "    metrics['f1_macro'] = f1_score(all_true_labels, all_predictions, average='macro')\n",
        "    metrics['f1_micro'] = f1_score(all_true_labels, all_predictions, average='micro')\n",
        "\n",
        "    # Precision and Recall\n",
        "    metrics['precision_weighted'] = precision_score(all_true_labels, all_predictions, average='weighted')\n",
        "    metrics['precision_macro'] = precision_score(all_true_labels, all_predictions, average='macro')\n",
        "\n",
        "\n",
        "    # Per-class metrics\n",
        "    unique_classes = np.unique(all_true_labels)\n",
        "    metrics['per_class_f1'] = f1_score(all_true_labels, all_predictions, average=None)\n",
        "    metrics['per_class_precision'] = precision_score(all_true_labels, all_predictions, average=None)\n",
        "\n",
        "\n",
        "    # Print summary\n",
        "\n",
        "    print(f\"\\n=== Validation Complete ===\")\n",
        "    print(f\"Total samples processed: {n_samples_processed}\")\n",
        "    print(f\"Overall accuracy: {metrics['accuracy']:.4f}\")\n",
        "    print(f\"F1-score (weighted): {metrics['f1_weighted']:.4f}\")\n",
        "    print(f\"F1-score (macro): {metrics['f1_macro']:.4f}\")\n",
        "    print(f\"F1-score (micro): {metrics['f1_micro']:.4f}\")\n",
        "\n",
        "    # Precision scores\n",
        "    print(f\"Precision (weighted): {metrics['precision_weighted']:.4f}\")\n",
        "    print(f\"Precision (macro): {metrics['precision_macro']:.4f}\")\n",
        "\n",
        "    print(\"\\nPer-class F1 scores:\")\n",
        "    for i, class_label in enumerate(unique_classes):\n",
        "        print(f\"  Class {class_label}: {metrics['per_class_f1'][i]:.4f}\")\n",
        "\n",
        "    # Return comprehensive results\n",
        "    return {\n",
        "        'metrics': metrics,\n",
        "        'predictions': all_predictions,\n",
        "        'true_labels': all_true_labels,\n",
        "        'n_samples': n_samples_processed\n",
        "    }\n",
        "\n",
        "# Run evaluation\n",
        "results = validate_model(\n",
        "    '/content/drive/MyDrive/svm3_with_scaler.pkl',\n",
        "    '/content/drive/MyDrive/svm3_scaler.pkl',\n",
        "    featureValidDirs,\n",
        "    targetValidDir,\n",
        "    12\n",
        ")"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "S09oOjclG67y",
      "metadata": {
        "id": "S09oOjclG67y"
      },
      "source": []
    },
    {
      "cell_type": "markdown",
      "id": "DReGUGYtqKax",
      "metadata": {
        "id": "DReGUGYtqKax"
      },
      "source": [
        "# PreProcessing Test Data In Batches"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 27,
      "id": "bIJhc-OZqM6l",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "bIJhc-OZqM6l",
        "outputId": "ac827093-d067-40b5-b1d2-149922b08e79"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Batches to Parquet: 100%|██████████| 4/4 [00:11<00:00,  2.85s/it]\n",
            "Batches to Parquet: 100%|██████████| 4/4 [00:03<00:00,  1.13it/s]\n",
            "Batches to Parquet: 100%|██████████| 4/4 [00:16<00:00,  4.14s/it]\n",
            "Batches to Parquet: 100%|██████████| 4/4 [00:02<00:00,  1.79it/s]\n"
          ]
        }
      ],
      "source": [
        "featureTestDirs = [\n",
        "    \"/content/svmBatches/bt1\",\n",
        "    \"/content/svmBatches/bt2\",\n",
        "    \"/content/svmBatches/bt3\"\n",
        "]\n",
        "\n",
        "targetTestDir = \"/content/svmBatches/bt0\"\n",
        "\n",
        "def batch_parquet(parsedDataset, featnum, features=[], batch_size=1000, max_records=None):\n",
        "\n",
        "  batch_files = []\n",
        "  total_recs = 0\n",
        "  batch_idx = 0\n",
        "\n",
        "  # Convert dataset to batches\n",
        "  batched_dataset = parsedDataset.batch(batch_size)\n",
        "  n_batches = totalTestRecs // batch_size\n",
        "\n",
        "  for batch in tqdm(batched_dataset,total=n_batches,desc=\"Batches to Parquet\"):\n",
        "    # Process each batch\n",
        "    batch_dict = {key: [] for key in features}\n",
        "\n",
        "    # Get batch size (might be smaller for the last batch)\n",
        "    current_batch_size = tf.shape(next(iter(batch.values())))[0].numpy()\n",
        "\n",
        "    # Extract features for each record in the batch\n",
        "    for i in range(current_batch_size):\n",
        "      for key in features:\n",
        "\n",
        "        # Handle different types of features\n",
        "        feature = batch[key][i]\n",
        "        if isinstance(feature, tf.Tensor):\n",
        "          value = feature.numpy()\n",
        "\n",
        "          # Convert bytes to string if applicable\n",
        "          if isinstance(value, bytes):\n",
        "            value = value.decode('utf-8')\n",
        "        else:\n",
        "            value = feature\n",
        "        batch_dict[key].append(value)\n",
        "\n",
        "    # Create DataFrame for this batch and append to list\n",
        "    df_batch = pd.DataFrame(batch_dict)\n",
        "\n",
        "    # Write to parquet\n",
        "    batch_file = f\"/content/svmBatches/bt{featnum}/batch_{batch_idx}.parquet\"\n",
        "    batch_idx += 1\n",
        "    df_batch.to_parquet(batch_file)\n",
        "    batch_files.append(batch_file)\n",
        "\n",
        "    # Update record count\n",
        "    total_recs += current_batch_size\n",
        "\n",
        "    # Check if we've reached the maximum number of records\n",
        "    if max_records is not None and total_recs >= max_records:\n",
        "        break\n",
        "  return\n",
        "\n",
        "# process features\n",
        "batch_parquet(testDS, 1, feat1, batch_size=1000,max_records=totalTestRecs)\n",
        "batch_parquet(testDS, 2, feat2, batch_size=1000,max_records=totalTestRecs)\n",
        "batch_parquet(testDS, 3, feat3, batch_size=1000,max_records=totalTestRecs)\n",
        "\n",
        "# process target\n",
        "batch_parquet(testDS, 0, target, batch_size=1000,max_records=totalTestRecs)\n"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Test Model"
      ],
      "metadata": {
        "id": "Yu70euomT24L"
      },
      "id": "Yu70euomT24L"
    },
    {
      "cell_type": "code",
      "execution_count": 30,
      "id": "cTZ3D5Kiq6Z3",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "cTZ3D5Kiq6Z3",
        "outputId": "75651cc0-35ba-4019-affa-a08e69939d2e"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\n",
            "=== Starting Test ===\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Test Chunk 0: 100%|██████████| 5/5 [00:18<00:00,  3.64s/it]\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Predicting on 4096 samples...\n",
            "Chunk accuracy: 0.1086\n",
            "\n",
            "=== Test Complete ===\n",
            "Total samples processed: 4096\n",
            "Overall accuracy: 0.1086\n",
            "F1-score (weighted): 0.1159\n",
            "F1-score (macro): 0.0971\n",
            "F1-score (micro): 0.1086\n",
            "Precision (weighted): 0.2673\n",
            "Precision (macro): 0.2425\n",
            "Recall (weighted): 0.1086\n",
            "Recall (macro): 0.0995\n",
            "\n",
            "Per-class F1 scores:\n",
            "  Class 0: 0.2306\n",
            "  Class 1: 0.0733\n",
            "  Class 2: 0.0633\n",
            "  Class 3: 0.0365\n",
            "  Class 4: 0.0101\n",
            "  Class 5: 0.1674\n",
            "  Class 6: 0.2305\n",
            "  Class 7: 0.0584\n",
            "  Class 8: 0.1609\n",
            "  Class 10: 0.0000\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/usr/local/lib/python3.11/dist-packages/sklearn/metrics/_classification.py:1565: UndefinedMetricWarning: Recall is ill-defined and being set to 0.0 in labels with no true samples. Use `zero_division` parameter to control this behavior.\n",
            "  _warn_prf(average, modifier, f\"{metric.capitalize()} is\", len(result))\n",
            "/usr/local/lib/python3.11/dist-packages/sklearn/metrics/_classification.py:1565: UndefinedMetricWarning: Recall is ill-defined and being set to 0.0 in labels with no true samples. Use `zero_division` parameter to control this behavior.\n",
            "  _warn_prf(average, modifier, f\"{metric.capitalize()} is\", len(result))\n",
            "/usr/local/lib/python3.11/dist-packages/sklearn/metrics/_classification.py:1565: UndefinedMetricWarning: Recall is ill-defined and being set to 0.0 in labels with no true samples. Use `zero_division` parameter to control this behavior.\n",
            "  _warn_prf(average, modifier, f\"{metric.capitalize()} is\", len(result))\n"
          ]
        }
      ],
      "source": [
        "\n",
        "def test_model(model_path,scaler_path, features_dir, target_dir, num_batches, batch_size=12):\n",
        "\n",
        "    model = joblib.load(model_path)\n",
        "    # Check if it's a tuple and extract the model\n",
        "    if isinstance(model, tuple):\n",
        "        # Assume the model is the first element\n",
        "        model = model[0]\n",
        "    else:\n",
        "        model = model\n",
        "    scaler = joblib.load(scaler_path)\n",
        "    # Track progress\n",
        "    all_predictions = []\n",
        "    all_true_labels = []\n",
        "    n_samples_processed = 0\n",
        "\n",
        "    print(f\"\\n=== Starting Test ===\")\n",
        "\n",
        "    for chunk_start in range(0, num_batches, batch_size):\n",
        "        chunk_end = min(chunk_start + batch_size, num_batches)\n",
        "        chunk_indices = range(chunk_start, chunk_end)\n",
        "\n",
        "        X_chunk_list = []\n",
        "        y_chunk_list = []\n",
        "\n",
        "        # Load batches in this chunk\n",
        "        for batch_idx in tqdm(chunk_indices, desc=f\"Test Chunk {chunk_start//batch_size}\"):\n",
        "            X_batch, y_batch = load_batch_from_multiple_dirs(batch_idx, features_dir, target_dir)\n",
        "\n",
        "            if X_batch is not None and y_batch is not None:\n",
        "                X_chunk_list.append(X_batch)\n",
        "                y_chunk_list.append(y_batch)\n",
        "        gc.collect()\n",
        "\n",
        "        if X_chunk_list:\n",
        "            # Stack all batches in chunk\n",
        "            X_chunk = np.vstack(X_chunk_list)\n",
        "            y_chunk = np.concatenate(y_chunk_list)\n",
        "\n",
        "            # Scale features using scaler\n",
        "            X_chunk_scaled = scaler.transform(X_chunk)\n",
        "\n",
        "            # Make predictions\n",
        "            print(f\"Predicting on {X_chunk.shape[0]} samples...\")\n",
        "            chunk_predictions = model.predict(X_chunk_scaled)\n",
        "\n",
        "            # Collect results\n",
        "            all_predictions.extend(chunk_predictions)\n",
        "            all_true_labels.extend(y_chunk)\n",
        "            n_samples_processed += X_chunk.shape[0]\n",
        "\n",
        "            # Calculate chunk accuracy\n",
        "            chunk_acc = accuracy_score(y_chunk, chunk_predictions)\n",
        "            print(f\"Chunk accuracy: {chunk_acc:.4f}\")\n",
        "\n",
        "            del X_chunk, X_chunk_scaled, y_chunk, chunk_predictions\n",
        "            gc.collect()\n",
        "\n",
        "    # Convert to numpy arrays for final metrics\n",
        "    all_predictions = np.array(all_predictions)\n",
        "    all_true_labels = np.array(all_true_labels)\n",
        "    metrics = {}\n",
        "\n",
        "    # Basic metrics\n",
        "    metrics['accuracy'] = accuracy_score(all_true_labels, all_predictions)\n",
        "\n",
        "    # F1 scores\n",
        "    metrics['f1_weighted'] = f1_score(all_true_labels, all_predictions, average='weighted')\n",
        "    metrics['f1_macro'] = f1_score(all_true_labels, all_predictions, average='macro')\n",
        "    metrics['f1_micro'] = f1_score(all_true_labels, all_predictions, average='micro')\n",
        "\n",
        "    # Precision and Recall\n",
        "    metrics['precision_weighted'] = precision_score(all_true_labels, all_predictions, average='weighted')\n",
        "    metrics['recall_weighted'] = recall_score(all_true_labels, all_predictions, average='weighted')\n",
        "    metrics['precision_macro'] = precision_score(all_true_labels, all_predictions, average='macro')\n",
        "    metrics['recall_macro'] = recall_score(all_true_labels, all_predictions, average='macro')\n",
        "\n",
        "    # Per-class metrics\n",
        "    unique_classes = np.unique(all_true_labels)\n",
        "    metrics['per_class_f1'] = f1_score(all_true_labels, all_predictions, average=None)\n",
        "    metrics['per_class_precision'] = precision_score(all_true_labels, all_predictions, average=None)\n",
        "    metrics['per_class_recall'] = recall_score(all_true_labels, all_predictions, average=None)\n",
        "\n",
        "    # Print summary\n",
        "\n",
        "    print(f\"\\n=== Test Complete ===\")\n",
        "    print(f\"Total samples processed: {n_samples_processed}\")\n",
        "    print(f\"Overall accuracy: {metrics['accuracy']:.4f}\")\n",
        "    print(f\"F1-score (weighted): {metrics['f1_weighted']:.4f}\")\n",
        "    print(f\"F1-score (macro): {metrics['f1_macro']:.4f}\")\n",
        "    print(f\"F1-score (micro): {metrics['f1_micro']:.4f}\")\n",
        "\n",
        "    # Precision scores\n",
        "    print(f\"Precision (weighted): {metrics['precision_weighted']:.4f}\")\n",
        "    print(f\"Precision (macro): {metrics['precision_macro']:.4f}\")\n",
        "\n",
        "\n",
        "    # Recall scores\n",
        "    print(f\"Recall (weighted): {metrics['recall_weighted']:.4f}\")\n",
        "    print(f\"Recall (macro): {metrics['recall_macro']:.4f}\")\n",
        "\n",
        "    print(\"\\nPer-class F1 scores:\")\n",
        "    for i, class_label in enumerate(unique_classes):\n",
        "        print(f\"  Class {class_label}: {metrics['per_class_f1'][i]:.4f}\")\n",
        "\n",
        "    # Return comprehensive results\n",
        "    return {\n",
        "        'metrics': metrics,\n",
        "        'predictions': all_predictions,\n",
        "        'true_labels': all_true_labels,\n",
        "        'n_samples': n_samples_processed\n",
        "    }\n",
        "\n",
        "# Run test evaluation\n",
        "results = test_model(\n",
        "    '/content/drive/MyDrive/svm3_with_scaler.pkl',\n",
        "    '/content/drive/MyDrive/svm3_scaler.pkl',\n",
        "    featureTestDirs,\n",
        "    targetTestDir,\n",
        "    5\n",
        ")"
      ]
    }
  ],
  "metadata": {
    "colab": {
      "machine_shape": "hm",
      "provenance": []
    },
    "kernelspec": {
      "display_name": "svm",
      "language": "python",
      "name": "svm"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.12.10"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 5
}